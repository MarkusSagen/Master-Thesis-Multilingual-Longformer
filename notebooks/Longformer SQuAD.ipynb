{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From\n",
    "#https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb\n",
    "\n",
    "!pip install tqdm==4.52.0\n",
    "!pip install --upgrade torch torchvision\n",
    "!pip install --upgrade transformers datasets\n",
    "!pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make AutoModel and AutoTokenizer\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n",
    "from typing import TypeVar\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import datasets\n",
    "from datasets import logging as DSlogging\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "DSlogging.set_verbosity_warning()\n",
    "\n",
    "#tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "MODEL_NAME = \"/workspace/models/RoBERTa_Long_seed_1337/RoBERTa_Long_seed_1337-4096-lm\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_correct_alignement(context: str, answer) -> Tuple[int, int]:\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx       # When the gold label position is good\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "        \n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example): \n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer.encode_plus(example['question'], example['context'], pad_to_max_length=True, max_length=512, truncation=True)\n",
    "    context_encodings = tokenizer.encode_plus(example['context'])\n",
    "    \n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    # this will give us the position of answer span in the context text\n",
    "    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\n",
    "    start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "    end_positions_context = context_encodings.char_to_token(end_idx-1)\n",
    "\n",
    "    # here we will compute the start and end position of the answer in the whole example\n",
    "    # as the example is encoded like this <s> question</s></s> context</s>\n",
    "    # and we know the postion of the answer in the context\n",
    "    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "    # this will give us the position of the answer span in whole example \n",
    "    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "    start_positions = start_positions_context + sep_idx + 1\n",
    "    end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "    if end_positions > 512:\n",
    "        start_positions, end_positions = 0, 0\n",
    "\n",
    "    encodings.update({'start_positions': start_positions,\n",
    "                      'end_positions': end_positions,\n",
    "                      'attention_mask': encodings['attention_mask']})\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def convert_to_features_map(example, tokenizer): \n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer.encode_plus(example['question'], example['context'], pad_to_max_length=True, max_length=512, truncation=True)\n",
    "    context_encodings = tokenizer.encode_plus(example['context'])\n",
    "    \n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    # this will give us the position of answer span in the context text\n",
    "    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\n",
    "    start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "    end_positions_context = context_encodings.char_to_token(end_idx-1)\n",
    "\n",
    "    # here we will compute the start and end position of the answer in the whole example\n",
    "    # as the example is encoded like this <s> question</s></s> context</s>\n",
    "    # and we know the postion of the answer in the context\n",
    "    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "    # this will give us the position of the answer span in whole example \n",
    "    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "    start_positions = start_positions_context + sep_idx + 1\n",
    "    end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "    if end_positions > 512:\n",
    "        start_positions, end_positions = 0, 0\n",
    "\n",
    "    encodings.update({'start_positions': start_positions,\n",
    "                      'end_positions': end_positions,\n",
    "                      'attention_mask': encodings['attention_mask']})\n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_features_loop(dataset, tokenizer): \n",
    "    for example in dataset:    \n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "        encodings = tokenizer.encode_plus(example['question'], example['context'], pad_to_max_length=True, max_length=512, truncation=True)\n",
    "        context_encodings = tokenizer.encode_plus(example['context'])\n",
    "\n",
    "        # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "        # this will give us the position of answer span in the context text\n",
    "        start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\n",
    "        start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "        end_positions_context = context_encodings.char_to_token(end_idx-1)\n",
    "\n",
    "        # here we will compute the start and end position of the answer in the whole example\n",
    "        # as the example is encoded like this <s> question</s></s> context</s>\n",
    "        # and we know the postion of the answer in the context\n",
    "        # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "        # this will give us the position of the answer span in whole example \n",
    "        sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 512:\n",
    "            start_positions, end_positions = 0, 0\n",
    "\n",
    "        encodings.update({'start_positions': start_positions,\n",
    "                          'end_positions': end_positions,\n",
    "                          'attention_mask': encodings['attention_mask']})\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# load train and validation split of squad\n",
    "train_dataset  = datasets.load_dataset('squad', split='train')\n",
    "valid_dataset = datasets.load_dataset('squad', split='validation')\n",
    "\n",
    "train_dataset = train_dataset.map(convert_to_features)\n",
    "valid_dataset = valid_dataset.map(convert_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it a datasets.arrow_dataset.Dataset to make to tensor for training\n",
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Transform datasets into tensors for training\n",
    "\n",
    "Set the tensor type and the columns which the dataset should return  \n",
    "This converts the format of the dataset to `torch`, `tf` or `pandas`  \n",
    "This allows the format of the model to be used when training a torch model  \n",
    "  \n",
    "- `type` define the return type for our dataset `__getitem__` method and is one of `[None, 'numpy', 'pandas', 'torch', 'tensorflow']` (`None` means return python objects), and\n",
    "- `columns` define the columns returned by `__getitem__` and takes the name of a column in the dataset or a list of columns to return (`None` means return all columns).  \n",
    "  \n",
    "  \n",
    "**NOTE**: Features are note removed from the dataset, just not passed when calling `__getitem__`.\n",
    "To go back to the dataset format (Needed when evaluating or done training):\n",
    "``` python\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
    "\n",
    "# Dataset to Tensor\n",
    "def convert_to_features(batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    input_pairs = list(zip())\n",
    "    encodings = tokenizer(batch['context'], batch['question'], truncation=True)\n",
    "\n",
    "    # Compute start and end tokens for labels\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, answer in enumerate(batch['answers']):\n",
    "        first_char = answer['answer_start'][0]\n",
    "        last_char = first_char + len(answer['text'][0]) - 1\n",
    "        start_positions.append(encodings.char_to_token(i, first_char))\n",
    "        end_positions.append(encodings.char_to_token(i, last_char))\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "data = datasets.load_dataset('squad', split=\"validation\")\n",
    "dataset.map(convert_to_features, batched=True)\n",
    "print(\"column_names\", encoded_dataset.column_names)\n",
    "print(\"start_positions\", encoded_dataset[:5]['start_positions'])\n",
    "\n",
    "# Convert to tensor format\n",
    "columns_to_return = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "data.set_format(type='torch', columns=columns_to_return)\n",
    "\n",
    "# Our dataset indexing output is now ready for being used in a pytorch dataloader\n",
    "pprint(data[1], compact=True)\n",
    "\n",
    "# Show that features are still there, just not directly callable\n",
    "print(data.column_names)\n",
    "\n",
    "# Convert back from Tensor to Dataset: `.reset_format()` \n",
    "# or call `.set_format()` with no arguments\n",
    "data.reset_format()\n",
    "pprint(data[1], compact=True)\n",
    "```\n",
    "========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach the dataset, so we can load it directly for training\n",
    "\"\"\"\n",
    "SQUAD_DIR = \"/workspace/data/SQuAD\"\n",
    "if not os.path.isdir(SQUAD_DIR):\n",
    "    os.mkdir(SQUAD_DIR)\n",
    "    print(f\"Creating data dir: {SQUAD_DIR}\")\n",
    "\n",
    "torch.save(train_dataset, f'{SQUAD_DIR}/train_data.pt')\n",
    "torch.save(valid_dataset, f'{SQUAD_DIR}/valid_data.pt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from here if saved dataset is already made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        #def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "    \n",
    "\n",
    "class DataCollector():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\" + \"\\tStarting Training and loading data\\t\"+ \" =\")\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\" + \"\\tInitialization\" + \"\\t\"*4 + \" =\")\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\"*50 +\"\\n\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\" + \"\\tStarting preparing data \\t\\t\"+ \" =\")\n",
    "print(\"=\" + \"\\t\"*6 + \" =\")\n",
    "print(\"=\"*50 +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add args\n",
    "SQUAD_DIR = \"/workspace/data/SQuAD\"\n",
    "\n",
    "args_dict = {\n",
    "  \"n_gpu\": 1,\n",
    "  \"model_name_or_path\": f\"{MODEL_NAME}\",    #'allenai/longformer-base-4096',\n",
    "    \"train_file_path\": f\"{SQUAD_DIR}/train_data.pt\",\n",
    "    \"val_file_path\": f\"{SQUAD_DIR}/valid_data.pt\",\n",
    "  \"max_len\": 512 ,\n",
    "  \"output_dir\": './models',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_gpu_train_batch_size\": 1, #8,\n",
    "  \"per_gpu_eval_batch_size\": 1, # 8,\n",
    "  \"gradient_accumulation_steps\": 2, # 16,\n",
    "  \"learning_rate\": 1e-4,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"do_train\": True,\n",
    "  \"max_steps\": 10,                            #\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "seed = 404\n",
    "# \"evaluation_strategy\": \"steps\",\n",
    "# \"overwrite_output_dir\": True,\n",
    "SQUAD_DIR = \"/workspace/data/SQuAD\"\n",
    "MODEL_DIR = \"/workspace/models\"\n",
    "LOG_DIR = \"/workspace/logs\"\n",
    "args_dict = {\n",
    "    \"output_dir\": f'{MODEL_DIR}/Longformer-4094-squad_seed_{seed}',\n",
    "    \"logging_dir\": f'{LOG_DIR}/Longformer-4094-squad_seed_{seed}',\n",
    "    \"train_file_path\": f\"{SQUAD_DIR}/train_data.pt\",\n",
    "    \"val_file_path\": f\"{SQUAD_DIR}/valid_data.pt\",\n",
    "    \"seed\": seed,\n",
    "    \"fp16\": True,\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"overwrite_output_dir\": True, \n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 10,\n",
    "    \"do_eval\": True,\n",
    "    \"do_train\": True,\n",
    "    \"n_gpu\": 1,\n",
    "    \"model_name_or_path\": 'allenai/longformer-base-4096',\n",
    "    \"max_len\": 512 ,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"max_steps\": 3,\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('args.json', 'w') as f:\n",
    "    json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add logging to file\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    val_file_path: Optional[str] = field(\n",
    "        default='valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    max_len: Optional[int] = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Max input length for the source text\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#def main():\n",
    "\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "# we will load the arguments from a json file, \n",
    "# make sure you save the arguments in at ./args.json\n",
    "# TODO replace\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "# model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=True\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "# TODO Tested to remove\n",
    "# Get datasets\n",
    "print('loading data')\n",
    "train_dataset  = torch.load(data_args.train_file_path)\n",
    "valid_dataset = torch.load(data_args.val_file_path)\n",
    "print('loading done')\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=DummyDataCollator(),\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    eval_output = trainer.evaluate()\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(eval_output.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "\n",
    "    results.update(eval_output)\n",
    "\n",
    "\n",
    "#return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQuAD evaluation script. Modifed slightly for this notebook\n",
    "\n",
    "# https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\n",
    "# https://github.com/huggingface/transformers/tree/master/examples/question-answering\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "        total += 1\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('./models')\n",
    "model = LongformerForQuestionAnswering.from_pretrained('./models')\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up dataloaded for batched evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset = torch.load('/workspace/data/SQuAD/valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted_answers = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        start_scores, end_scores = model(input_ids=batch['input_ids'].cuda(),\n",
    "                                  attention_mask=batch['attention_mask'].cuda())\n",
    "        for i in range(start_scores.shape[0]):\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores[i]) : torch.argmax(end_scores[i])+1])\n",
    "            ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "            answer = tokenizer.decode(ans_ids)\n",
    "            predicted_answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predicitons\n",
    "**Importaint!**: Change the valid_dataset back from tensor format, to default python dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "# valid_dataset = nlp.load_dataset('squad', split='validation')\n",
    "for ref, pred_answer in zip(valid_dataset, predicted_answers):\n",
    "    actual_answer = ref['answers']['text']\n",
    "    predictions.append(pred_answer)\n",
    "    references.append(actual_answer)\n",
    "    \n",
    "evaluate(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.predict(valid_dataset)\n",
    "# output:\n",
    "# PredictionOutput(predictions=None, label_ids=None, metrics={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/data/SQuAD'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname('/workspace/data/SQuAD/train-v1.1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old way to convert text to Model predictions.  \n",
    "Given tokenizer and pretrained QA-model\n",
    "\n",
    "``` python\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, \n",
    "                                   add_special_tokens=True, \n",
    "                                   return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual way to make answer prediction\n",
    "This is mainly to illustrate how the basic prediction is made:  \n",
    "``` python\n",
    "input_ids = valid_dataset[0]['input_ids'].tolist()\n",
    "start = valid_dataset['start_positions'][0]\n",
    "end   = valid_dataset['end_positions'][0]\n",
    "print(start, end)\n",
    "\n",
    "# Old way - May not work anymore\n",
    "prediciton = tok.convert_tokens_to_string(tok.convert_ids_to_tokens(input_ids2[start:end]))\n",
    "\n",
    "# New way\n",
    "# Cleaning tokens need to be made. Left in for illustration and easier separation.\n",
    "prediction = tok.decode(input_ids[start:end]) # But!!! includes unwanted spaces\n",
    "print(prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using A fine-tuned model in action ðŸš€\n",
    "The trained model is available on Huggingface hub if you want to play with it   \n",
    "You can find the model [here](https://huggingface.co/valhalla/longformer-base-4096-finetuned-squadv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "\n",
    "text = \"Huggingface has democratized NLP. Huge thanks to Huggingface for this.\"\n",
    "question = \"What has Huggingface done ?\"\n",
    "encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "# output => democratized NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or use Huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Roberta pretrained model for Squad\n",
    "# https://huggingface.co/deepset/roberta-base-squad2\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.modeling_auto import AutoModelForQuestionAnswering, RobertaForMaskedLM\n",
    "from transformers.tokenization_auto import AutoTokenizer\n",
    "\n",
    "#model_name = \"deepset/roberta-base-squad2\"\n",
    "Pipeline_model_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n",
    "\n",
    "nlp = pipeline('question-answering', model=Pipeline_model_name, tokenizer=Pipeline_model_name)\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "\"\"\"\n",
    "\n",
    "# yields very low scores compared to Huggingfaces own QA pipeline\n",
    "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/geblanco/mc_transformers/blob/master/mc_transformers/mc_transformers.py\n",
    "def pair_predictions_with_ids(results, data_collator):\n",
    "    return PredictionOutputWithIds(\n",
    "        predictions=results.predictions,\n",
    "        label_ids=results.label_ids,\n",
    "        example_ids=data_collator.example_ids,\n",
    "        metrics=results.metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    result = trainer.predict(eval_dataset)\n",
    "    if trainer.is_world_master():\n",
    "        result = pair_predictions_with_ids(result, data_collator)\n",
    "        save_results(\n",
    "            processor, result, all_args, split=Split.dev\n",
    "        )\n",
    "        results['eval'] = result\n",
    "        data_collator.drop_ids()\n",
    "\n",
    "if training_args.do_predict:\n",
    "    logger.info(\"*** Test ***\")\n",
    "    result = trainer.predict(test_dataset)\n",
    "    if trainer.is_world_master():\n",
    "        result = pair_predictions_with_ids(result, data_collator)\n",
    "        save_results(\n",
    "            processor, result, all_args, split=Split.test\n",
    "        )\n",
    "        results['test'] = result\n",
    "        data_collator.drop_ids()\n",
    "\n",
    "#return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingfaces predict\n",
    "\n",
    "\n",
    "if args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        eval_examples = read_squad_examples(\n",
    "            input_file=args.predict_file, is_training=False, version_2_with_negative=args.version_2_with_negative)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples=eval_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            doc_stride=args.doc_stride,\n",
    "            max_query_length=args.max_query_length,\n",
    "            is_training=False)\n",
    "\n",
    "        logger.info(\"***** Running predictions *****\")\n",
    "        logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "        logger.info(\"  Batch size = %d\", args.predict_batch_size)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        all_results = []\n",
    "        logger.info(\"Start evaluating\")\n",
    "        for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\", disable=args.local_rank not in [-1, 0]):\n",
    "            if len(all_results) % 1000 == 0:\n",
    "                logger.info(\"Processing example: %d\" % (len(all_results)))\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "                end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "                eval_feature = eval_features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                all_results.append(RawResult(unique_id=unique_id,\n",
    "                                             start_logits=start_logits,\n",
    "                                             end_logits=end_logits))\n",
    "        output_prediction_file = os.path.join(args.output_dir, \"predictions.json\")\n",
    "        output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions.json\")\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds.json\")\n",
    "        write_predictions(eval_examples, eval_features, all_results,\n",
    "                          args.n_best_size, args.max_answer_length,\n",
    "                          args.do_lower_case, output_prediction_file,\n",
    "                          output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                          args.version_2_with_negative, args.null_score_diff_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Datasets Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the library. We typically only need at most four methods:\n",
    "from datasets import list_datasets, list_metrics, load_dataset, load_metric\n",
    "# Let's import a fast tokenizer that can work on batched inputs\n",
    "# (the 'Fast' tokenizers in HuggingFace)\n",
    "import datasets\n",
    "from datasets import logging as dataset_logging\n",
    "from pprint import pprint\n",
    "#dataset_logging.set_verbosity_info()\n",
    "dataset_logging.set_verbosity_warning()\n",
    "\n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, logging as transformers_logging\n",
    "\n",
    "transformers_logging.set_verbosity_warning()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load our training dataset and tokenizer\n",
    "dataset = load_dataset('squad')\n",
    "train_dataset = load_dataset('squad', split=\"train\")\n",
    "valid_dataset = load_dataset('squad', split=\"validation\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx       # When the gold label position is good\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer(example_batch['context'], example_batch['question'], truncation=True)\n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
    "        start_idx, end_idx = get_correct_alignement(context, answer)\n",
    "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
    "        end_positions.append(encodings.char_to_token(i, end_idx-1))\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "encoded_dataset = dataset.map(convert_to_features, batched=True)\n",
    "train_dataset.map(convert_to_features, batched=True)\n",
    "valid_dataset.map(convert_to_features, batched=True)\n",
    "\n",
    "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "encoded_dataset.set_format(type='torch', columns=columns)\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "# Let's do dynamic batching (pad on the fly with our own collate_fn)\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, return_tensors='pt')\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset['train'], collate_fn=collate_fn, batch_size=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('distilbert-base-cased', return_dict=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "from datasets import load_metric\n",
    "# You need to give the total number of parallel python processes (num_process) and the id of each process (process_id)\n",
    "squad_metric = datasets.load_metric('squad')\n",
    "\n",
    "\n",
    "# Now let's train our model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model.train().to(device)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    batch.to(device)\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    print(f'Step {i} - loss: {loss:.3}')\n",
    "    \n",
    "    pprint(outputs)\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How others have used padding adn truncation w datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from unittest.mock import patch\n",
    "\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from transformers.file_utils import is_datasets_available\n",
    "from transformers.testing_utils import (\n",
    "    TestCasePlus,\n",
    "    execute_subprocess_async,\n",
    "    get_gpu_count,\n",
    "    require_torch_non_multi_gpu_but_fix_me,\n",
    "    slow,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "MARIAN_MODEL = \"sshleifer/student_marian_en_ro_6_1\"\n",
    "\n",
    "\n",
    "class TestFinetuneTrainer(TestCasePlus):\n",
    "    def test_finetune_trainer(self):\n",
    "        output_dir = self.run_trainer(1, \"12\", MBART_TINY, 1)\n",
    "        logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n",
    "        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n",
    "        first_step_stats = eval_metrics[0]\n",
    "        assert \"eval_bleu\" in first_step_stats\n",
    "\n",
    "    @slow\n",
    "    def test_finetune_trainer_slow(self):\n",
    "        # There is a missing call to __init__process_group somewhere\n",
    "        output_dir = self.run_trainer(eval_steps=2, max_len=\"128\", model_name=MARIAN_MODEL, num_train_epochs=10)\n",
    "\n",
    "        # Check metrics\n",
    "        logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n",
    "        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n",
    "        first_step_stats = eval_metrics[0]\n",
    "        last_step_stats = eval_metrics[-1]\n",
    "\n",
    "        assert first_step_stats[\"eval_bleu\"] < last_step_stats[\"eval_bleu\"]  # model learned nothing\n",
    "        assert isinstance(last_step_stats[\"eval_bleu\"], float)\n",
    "\n",
    "        # test if do_predict saves generations and metrics\n",
    "        contents = os.listdir(output_dir)\n",
    "        contents = {os.path.basename(p) for p in contents}\n",
    "        assert \"test_generations.txt\" in contents\n",
    "        assert \"test_results.json\" in contents\n",
    "\n",
    "    @slow\n",
    "    @require_torch_non_multi_gpu_but_fix_me\n",
    "    def test_finetune_bert2bert(self):\n",
    "        if not is_datasets_available():\n",
    "            return\n",
    "\n",
    "        import datasets\n",
    "\n",
    "        bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"prajjwal1/bert-tiny\", \"prajjwal1/bert-tiny\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n",
    "        bert2bert.config.eos_token_id = tokenizer.sep_token_id\n",
    "        bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "        bert2bert.config.max_length = 128\n",
    "\n",
    "        train_dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "        val_dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:1%]\")\n",
    "\n",
    "        train_dataset = train_dataset.select(range(32))\n",
    "        val_dataset = val_dataset.select(range(16))\n",
    "\n",
    "        rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "        batch_size = 4\n",
    "\n",
    "        def _map_to_encoder_decoder_inputs(batch):\n",
    "            # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "            inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "            outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "            batch[\"input_ids\"] = inputs.input_ids\n",
    "            batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "            batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "            batch[\"labels\"] = outputs.input_ids.copy()\n",
    "            batch[\"labels\"] = [\n",
    "                [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
    "            ]\n",
    "            batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "            assert all([len(x) == 512 for x in inputs.input_ids])\n",
    "            assert all([len(x) == 128 for x in outputs.input_ids])\n",
    "\n",
    "            return batch\n",
    "\n",
    "        def _compute_metrics(pred):\n",
    "            labels_ids = pred.label_ids\n",
    "            pred_ids = pred.predictions\n",
    "\n",
    "            # all unnecessary tokens are removed\n",
    "            pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "            label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "            rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\n",
    "                \"rouge2\"\n",
    "            ].mid\n",
    "\n",
    "            return {\n",
    "                \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "                \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "                \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "            }\n",
    "\n",
    "        # map train dataset\n",
    "        train_dataset = train_dataset.map(\n",
    "            _map_to_encoder_decoder_inputs,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=[\"article\", \"highlights\"],\n",
    "        )\n",
    "        train_dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "        )\n",
    "\n",
    "        # same for validation dataset\n",
    "        val_dataset = val_dataset.map(\n",
    "            _map_to_encoder_decoder_inputs,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=[\"article\", \"highlights\"],\n",
    "        )\n",
    "        val_dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "        )\n",
    "\n",
    "        output_dir = self.get_auto_remove_tmp_dir()\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            predict_with_generate=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            do_train=True,\n",
    "            do_eval=True,\n",
    "            warmup_steps=0,\n",
    "            eval_steps=2,\n",
    "            logging_steps=2,\n",
    "        )\n",
    "\n",
    "        # instantiate trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=bert2bert,\n",
    "            args=training_args,\n",
    "            compute_metrics=_compute_metrics,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "        )\n",
    "\n",
    "        # start training\n",
    "        trainer.train()\n",
    "\n",
    "    def run_trainer(self, eval_steps: int, max_len: str, model_name: str, num_train_epochs: int):\n",
    "        data_dir = self.examples_dir / \"seq2seq/test_data/wmt_en_ro\"\n",
    "        output_dir = self.get_auto_remove_tmp_dir()\n",
    "        args = f\"\"\"\n",
    "            --model_name_or_path {model_name}\n",
    "            --data_dir {data_dir}\n",
    "            --output_dir {output_dir}\n",
    "            --overwrite_output_dir\n",
    "            --n_train 8\n",
    "            --n_val 8\n",
    "            --max_source_length {max_len}\n",
    "            --max_target_length {max_len}\n",
    "            --val_max_target_length {max_len}\n",
    "            --do_train\n",
    "            --do_eval\n",
    "            --do_predict\n",
    "            --num_train_epochs {str(num_train_epochs)}\n",
    "            --per_device_train_batch_size 4\n",
    "            --per_device_eval_batch_size 4\n",
    "            --learning_rate 3e-3\n",
    "            --warmup_steps 8\n",
    "            --evaluation_strategy steps\n",
    "            --predict_with_generate\n",
    "            --logging_steps 0\n",
    "            --save_steps {str(eval_steps)}\n",
    "            --eval_steps {str(eval_steps)}\n",
    "            --sortish_sampler\n",
    "            --label_smoothing 0.1\n",
    "            --adafactor\n",
    "            --task translation\n",
    "            --tgt_lang ro_RO\n",
    "            --src_lang en_XX\n",
    "        \"\"\".split()\n",
    "        # --eval_beams  2\n",
    "\n",
    "        n_gpu = get_gpu_count()\n",
    "        if n_gpu > 1:\n",
    "            distributed_args = f\"\"\"\n",
    "                -m torch.distributed.launch\n",
    "                --nproc_per_node={n_gpu}\n",
    "                {self.test_file_dir}/finetune_trainer.py\n",
    "            \"\"\".split()\n",
    "            cmd = [sys.executable] + distributed_args + args\n",
    "            execute_subprocess_async(cmd, env=self.get_env())\n",
    "        else:\n",
    "            # 0 or 1 gpu\n",
    "            testargs = [\"finetune_trainer.py\"] + args\n",
    "            with patch.object(sys, \"argv\", testargs):\n",
    "                main()\n",
    "\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
