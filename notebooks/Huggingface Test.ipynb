{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Huggingface Basics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'FutureWarning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 47446, 10, 2007, 13931, 878, 15, 3830, 11126, 38495, 1542, 1421, 15, 221, 6607, 271, 1499, 6063, 10228, 2]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilroberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "seq = \"Testing a simple sequence running on RoBERTa base model on Peltarion remote server\"\n",
    "print(tokenizer.encode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': {'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json',\n",
       "  'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json',\n",
       "  'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-vocab.json',\n",
       "  'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json',\n",
       "  'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json',\n",
       "  'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json'},\n",
       " 'merges_file': {'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt',\n",
       "  'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt',\n",
       "  'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-merges.txt',\n",
       "  'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt',\n",
       "  'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt',\n",
       "  'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where tokenizer files are stored\n",
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Testing',\n",
       " 'Ä a',\n",
       " 'Ä simple',\n",
       " 'Ä sequence',\n",
       " 'Ä running',\n",
       " 'Ä on',\n",
       " 'Ä Ro',\n",
       " 'BER',\n",
       " 'Ta',\n",
       " 'Ä base',\n",
       " 'Ä model',\n",
       " 'Ä on',\n",
       " 'Ä P',\n",
       " 'elt',\n",
       " 'ar',\n",
       " 'ion',\n",
       " 'Ä remote',\n",
       " 'Ä server',\n",
       " '</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing sentance: \n",
      " ['Testing', 'Ä a', 'Ä simple', 'Ä sequence', 'Ä running', 'Ä on', 'Ä Ro', 'BER', 'Ta', 'Ä base', 'Ä model', 'Ä on', 'Ä P', 'elt', 'ar', 'ion', 'Ä remote', 'Ä server']\n",
      "\n",
      "Printing encoding\n",
      " {'input_ids': [0, 47446, 10, 2007, 13931, 878, 15, 3830, 11126, 38495, 1542, 1421, 15, 221, 6607, 271, 1499, 6063, 10228, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Only input_ids:\n",
      " [0, 47446, 10, 2007, 13931, 878, 15, 3830, 11126, 38495, 1542, 1421, 15, 221, 6607, 271, 1499, 6063, 10228, 2]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sequence = tokenizer.tokenize(seq)\n",
    "encoded_input = tokenizer(seq)\n",
    "print(\"Printing sentance: \\n\", tokenized_sequence)\n",
    "print(\"\\nPrinting encoding\\n\", encoded_input)\n",
    "print(\"\\nOnly input_ids:\\n\", encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import dataclasses\n",
    "#import json\n",
    "import random\n",
    "#from dataclasses import dataclass\n",
    "#from typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf``\n",
    "    (if installed).\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding multiple sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "\n",
    "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since different lengths, we need to padd each sentance\n",
    "len(encoded_sequence_a), len(encoded_sequence_b)\n",
    "\n",
    "# Easiest is to truncate / Padd  automaticlly with the tokenizer\n",
    "# Send in the text sentances and add PADDING\n",
    "padded_seq = tokenizer([sequence_a, sequence_b], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance 0: \n",
      " [0, 713, 16, 10, 765, 13931, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "With Mask: \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Sentance 1: \n",
      " [0, 713, 16, 10, 1195, 251, 13931, 4, 85, 16, 23, 513, 1181, 87, 5, 13931, 83, 4, 2]\n",
      "With Mask: \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "padded_input_ids = padded_seq['input_ids']\n",
    "padded_attention_mask = padded_seq['attention_mask']\n",
    "\n",
    "# Notice that the shorter sentance has additional padding, with mask of 0 to indixate that those are simply paddings\n",
    "for idx, sentance in enumerate(padded_input_ids):\n",
    "    print(f\"Sentance {idx}: \\n\", padded_input_ids[idx])\n",
    "    print(f\"With Mask: \\n\", padded_attention_mask[idx])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task specific and unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transformer models encode words and tokens differently   \n",
    "For instance BERT [CLS], [SEP], [UNK], [MASK] tokens and more   \n",
    "   \n",
    "Notice how BERT and RoBERTa uses different tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This is a short sequence. [SEP] This is a rather long sequence. It is at least longer than the sequence A. [SEP]\n",
      "\n",
      "Token types: \n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# Show how RoBERTa/BERT encodes sentances into multiple ones\n",
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "encoded_seq = tokenizer(sequence_a, sequence_b)\n",
    "decoded_seq = tokenizer.decode(encoded_seq['input_ids'])\n",
    "print(decoded_seq)\n",
    "# In BERT, each sentance also a token indicating which sentance it belongs to, since modeling next sentance prediction during pretraining\n",
    "print(\"\\nToken types: \\n\", encoded_seq['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>This is a short sequence.</s></s>This is a rather long sequence. It is at least longer than the sequence A.</s>\n",
      "{'input_ids': [0, 713, 16, 10, 765, 13931, 4, 2, 2, 713, 16, 10, 1195, 251, 13931, 4, 85, 16, 23, 513, 1181, 87, 5, 13931, 83, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoded_seq = tokenizer(sequence_a, sequence_b)\n",
    "decoded_seq = tokenizer.decode(encoded_seq['input_ids'])\n",
    "print(decoded_seq)\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> This is a rather long sequence. It is at least longer than the sequence A.</s></s> Det hÃ¤r Ã¤r en mening pÃ¥ Svenska!</s>\n",
      "{'input_ids': [0, 3293, 83, 10, 43257, 4989, 40, 944, 3956, 5, 1650, 83, 99, 19713, 51713, 3501, 70, 40, 944, 3956, 62, 5, 2, 2, 579, 2496, 369, 22, 26213, 109, 46062, 38, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Show how RoBERTa encodes sentances into multiple ones\n",
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "sequence_sv = \"Det hÃ¤r Ã¤r en mening pÃ¥ Svenska!\"\n",
    "\n",
    "\"\"\"\n",
    "xlm-roberta-base, xlm-roberta-large, xlm-roberta-large-finetuned-conll02-dutch, xlm-roberta-large-finetuned-conll02-spanish, xlm-roberta-large-finetuned-conll03-english, xlm-roberta-large-finetuned-conll03-german\n",
    "\"\"\"\n",
    "XLM_NAME = 'xlm-roberta-base'\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(XLM_NAME)\n",
    "encoded_seq = tokenizer(sequence_b, sequence_sv)\n",
    "decoded_seq = tokenizer.decode(encoded_seq['input_ids'])\n",
    "# Notice that much larger tokens are used to encode the sentance!\n",
    "print(decoded_seq)\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures: \t ['XLMRobertaForMaskedLM']\n",
      "attention_probs_dropout_prob: \t 0.1\n",
      "bos_token_id: \t 0\n",
      "eos_token_id: \t 2\n",
      "hidden_act: \t gelu\n",
      "hidden_dropout_prob: \t 0.1\n",
      "hidden_size: \t 768\n",
      "initializer_range: \t 0.02\n",
      "intermediate_size: \t 3072\n",
      "layer_norm_eps: \t 1e-05\n",
      "max_position_embeddings: \t 514\n",
      "model_type: \t xlm-roberta\n",
      "num_attention_heads: \t 12\n",
      "num_hidden_layers: \t 12\n",
      "output_past: \t True\n",
      "pad_token_id: \t 1\n",
      "type_vocab_size: \t 1\n",
      "vocab_size: \t 250002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'architectures': ['XLMRobertaForMaskedLM'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'bos_token_id': 0,\n",
       " 'eos_token_id': 2,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-05,\n",
       " 'max_position_embeddings': 514,\n",
       " 'model_type': 'xlm-roberta',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'output_past': True,\n",
       " 'pad_token_id': 1,\n",
       " 'type_vocab_size': 1,\n",
       " 'vocab_size': 250002}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f\"{key}: \\t {value}\") for (key, value) in XLMRobertaConfig.get_config_dict(XLM_NAME)[0].items()]\n",
    "\n",
    "# easier\n",
    "config_easy = XLMRobertaConfig.from_pretrained(XLM_NAME)\n",
    "\n",
    "config = XLMRobertaConfig.get_config_dict(XLM_NAME)[0]\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX Position embeddings XLM-R:  514\n",
      "XLM-R embedding range: [0, 513]\n"
     ]
    }
   ],
   "source": [
    "print(\"MAX Position embeddings XLM-R: \", config['max_position_embeddings'])\n",
    "print(\"XLM-R embedding range: [0, {}]\".format(int(config['max_position_embeddings']) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification\n",
    "config_easy\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', config=config_easy)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', tensor([[    0,  1121,  2627,     6,  9366,  1665,    11,  4828,  9629,     6,\n",
       "           215,    25,    23,    10,  2391,     6,    16,  2633,  9977,  5895,\n",
       "           196,     4,     2,     2,  1121,  2627,     6,  9366,  1665,    11,\n",
       "          4828,  9629,     6,   215,    25,    23,    10,  2391,     6,    16,\n",
       "          2633,  9977,  5895,   196,     4,     2],\n",
       "        [    0,   243,    16, 18804,    19,    10, 20935,     8,    10,  7023,\n",
       "             4,     2,     2,   243,    16, 18804,   150,   547,    11,     5,\n",
       "           865,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1121,  2627,     6,  9366,  1665,    11,  4828,  9629,     6,\n",
       "            215,    25,    23,    10,  2391,     6,    16,  2633,  9977,  5895,\n",
       "            196,     4,     2,     2,  1121,  2627,     6,  9366,  1665,    11,\n",
       "           4828,  9629,     6,   215,    25,    23,    10,  2391,     6,    16,\n",
       "           2633,  9977,  5895,   196,     4,     2],\n",
       "         [    0,   243,    16, 18804,    19,    10, 20935,     8,    10,  7023,\n",
       "              4,     2,     2,   243,    16, 18804,   150,   547,    11,     5,\n",
       "            865,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k,v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[    0,  1121,  2627,     6,  9366,  1665,    11,  4828,  9629,     6,\n",
       "             215,    25,    23,    10,  2391,     6,    16,  2633,  9977,  5895,\n",
       "             196,     4,     2,     2,  1121,  2627,     6,  9366,  1665,    11,\n",
       "            4828,  9629,     6,   215,    25,    23,    10,  2391,     6,    16,\n",
       "            2633,  9977,  5895,   196,     4,     2],\n",
       "          [    0,   243,    16, 18804,    19,    10, 20935,     8,    10,  7023,\n",
       "               4,     2,     2,   243,    16, 18804,   150,   547,    11,     5,\n",
       "             865,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1]]]),\n",
       " 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.unsqueeze(0) for k,v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-R QA\n",
    "# https://huggingface.co/deepset/roberta-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# https://huggingface.co/deepset/xlm-roberta-large-squad2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"a-ware/xlmroberta-QA\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"a-ware/xlmroberta-QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuned for XQuAD?\n",
    "# https://huggingface.co/deepset/xlm-roberta-large-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeslc', 'ag_news', 'ai2_arc', 'allocine', 'anli', 'arcd', 'art', 'billsum', 'biomrc', 'blended_skill_talk', 'blimp', 'blog_authorship_corpus', 'bookcorpus', 'boolq', 'break_data', 'c4', 'cfq', 'civil_comments', 'clue', 'cmrc2018', 'cnn_dailymail', 'coarse_discourse', 'com_qa', 'common_gen', 'commonsense_qa', 'compguesswhat', 'conll2000', 'conll2003', 'coqa', 'cornell_movie_dialog', 'cos_e', 'cosmos_qa', 'crd3', 'crime_and_punish', 'csv', 'daily_dialog', 'definite_pronoun_resolution', 'discofuse', 'docred', 'doqa', 'drop', 'eli5', 'emo', 'emotion', 'empathetic_dialogues', 'eraser_multi_rc', 'esnli', 'event2Mind', 'fever', 'flores', 'fquad', 'gap', 'germeval_14', 'gigaword', 'glue', 'guardian_authorship', 'hans', 'hansards', 'hellaswag', 'hotpot_qa', 'hyperpartisan_news_detection', 'imdb', 'iwslt2017', 'jeopardy', 'json', 'kilt_tasks', 'kilt_wikipedia', 'kor_nli', 'lc_quad', 'librispeech_lm', 'lince', 'lm1b', 'math_dataset', 'math_qa', 'matinf', 'mlqa', 'mlsum', 'movie_rationales', 'ms_marco', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'mwsc', 'natural_questions', 'newsgroup', 'newsroom', 'openbookqa', 'openwebtext', 'opinosis', 'pandas', 'para_crawl', 'pg19', 'piaf', 'polyglot_ner', 'qa4mre', 'qa_zre', 'qangaroo', 'qanta', 'qasc', 'quarel', 'quartz', 'quora', 'quoref', 'race', 'reclor', 'reddit', 'reddit_tifu', 'reuters21578', 'rotten_tomatoes', 'scan', 'scicite', 'scientific_papers', 'scifact', 'sciq', 'scitail', 'search_qa', 'sentiment140', 'snli', 'social_bias_frames', 'social_i_qa', 'sogou_news', 'squad', 'squad_es', 'squad_it', 'squad_v1_pt', 'squad_v2', 'squadshifts', 'style_change_detection', 'super_glue', 'ted_hrlr', 'ted_multi', 'text', 'tiny_shakespeare', 'trec', 'trivia_qa', 'tydiqa', 'ubuntu_dialogs_corpus', 'web_of_science', 'web_questions', 'wiki40b', 'wiki_dpr', 'wiki_qa', 'wiki_snippets', 'wiki_split', 'wikihow', 'wikipedia', 'wikisql', 'wikitext', 'winogrande', 'wiqa', 'wmt14', 'wmt15', 'wmt16', 'wmt17', 'wmt18', 'wmt19', 'wmt_t2t', 'wnut_17', 'x_stance', 'xcopa', 'xnli', 'xquad', 'xsum', 'xtreme', 'yelp_polarity']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "dataset_list = list_datasets()\n",
    "print(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\n"
     ]
    }
   ],
   "source": [
    "# Load SQuAD\n",
    "\"\"\"\n",
    "[ \"NAME\", \"DATA\" ]\n",
    "\n",
    "{\n",
    "    \"id\":\"string\"\n",
    "    \"title\":\"string\"\n",
    "    \"context\":\"string\"\n",
    "    \"question\":\"string\"\n",
    "        \"answers\":{\n",
    "        \"[]\":{\n",
    "            \"text\":\"string\"\n",
    "            \"answer_start\":\"int32\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_train, data_val = load_dataset('squad').items()\n",
    "data_train = data_train[1]\n",
    "data_val = data_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}, num_rows: 87599)\n",
      "\n",
      "Dataset(features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}, num_rows: 10570)\n"
     ]
    }
   ],
   "source": [
    "print(data_train)\n",
    "print()\n",
    "print(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5733be284776f41900661182\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "id, title, context, question, answers, \n",
    "\"\"\"\n",
    "print(data_train['id'][0])\n",
    "print(data_train['context'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load via pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f8c1ce717d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# yields very low scores compared to Huggingfaces own QA pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"What is extractive question answering?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"What is a good example of a question answering dataset?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         ),\n\u001b[1;32m   1315\u001b[0m                     }\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m                 ]\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         ),\n\u001b[1;32m   1315\u001b[0m                     }\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m                 ]\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "### Load Roberta pretrained model for Squad\n",
    "# https://huggingface.co/deepset/roberta-base-squad2\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.modeling_auto import AutoModelForQuestionAnswering\n",
    "from transformers.tokenization_auto import AutoTokenizer\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "# nlp = pipeline('question-answering')\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "\"\"\"\n",
    "\n",
    "# yields very low scores compared to Huggingfaces own QA pipeline\n",
    "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6222440004348755, 'start': 34, 'end': 96, 'answer': 'the task of extracting an answer from a text given a question.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5115299820899963, 'start': 147, 'end': 161, 'answer': 'SQuAD dataset,'}\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline('question-answering')\n",
    "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load via models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7116c69dbdcb41b18b0091fa96200b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6e82467a034565b70fa6a56a07bcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6feb4e5da6745ef89d831d8952eb0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How many pretrained models are available in Transformers?\n",
      "Answer: over 32 +\n",
      "\n",
      "Question: What does Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "\n",
      "Question: Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2 . 0 and pytorch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # b) Load model & tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e086510bd53f4a0b983047f434aef1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae386616a5ae47d2aa2f283dca61fc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e694dd3d5984553bb4f9250f589d9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=260793700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How many pretrained models are available in Transformers?\n",
      "Answer: over 32 +\n",
      "\n",
      "Question: What does Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "\n",
      "Question: Transformers provides interoperability between which frameworks?\n",
      "Answer: TensorFlow 2 . 0 and PyTorch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # b) Load model & tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Here is an example of question answering using a model and a tokenizer. The process is the following:\n",
    "Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    "Define a text and a few questions.\n",
    "Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks\n",
    "Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "Compute the softmax of the result to get probabilities over the tokens\n",
    "Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    "Print the results\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # Equivalent to defining first a vector of the inputs, how long and where the sep token is located / called\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'How',\n",
       " 'many',\n",
       " 'pre',\n",
       " '##tra',\n",
       " '##ined',\n",
       " 'models',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " 'Transformers',\n",
       " '?',\n",
       " '[SEP]',\n",
       " '[UNK]',\n",
       " 'Transformers',\n",
       " '(',\n",
       " 'formerly',\n",
       " 'known',\n",
       " 'as',\n",
       " 'p',\n",
       " '##yt',\n",
       " '##or',\n",
       " '##ch',\n",
       " '-',\n",
       " 'transform',\n",
       " '##ers',\n",
       " 'and',\n",
       " 'p',\n",
       " '##yt',\n",
       " '##or',\n",
       " '##ch',\n",
       " '-',\n",
       " 'pre',\n",
       " '##tra',\n",
       " '##ined',\n",
       " '-',\n",
       " 'be',\n",
       " '##rt',\n",
       " ')',\n",
       " 'provides',\n",
       " 'general',\n",
       " '-',\n",
       " 'purpose',\n",
       " 'architecture',\n",
       " '##s',\n",
       " '(',\n",
       " 'B',\n",
       " '##ER',\n",
       " '##T',\n",
       " ',',\n",
       " 'GP',\n",
       " '##T',\n",
       " '-',\n",
       " '2',\n",
       " ',',\n",
       " 'R',\n",
       " '##o',\n",
       " '##BE',\n",
       " '##RT',\n",
       " '##a',\n",
       " ',',\n",
       " 'X',\n",
       " '##LM',\n",
       " ',',\n",
       " 'Di',\n",
       " '##st',\n",
       " '##il',\n",
       " '##B',\n",
       " '##ert',\n",
       " ',',\n",
       " 'X',\n",
       " '##L',\n",
       " '##Net',\n",
       " 'â€¦',\n",
       " ')',\n",
       " 'for',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Understanding',\n",
       " '(',\n",
       " 'NL',\n",
       " '##U',\n",
       " ')',\n",
       " 'and',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Generation',\n",
       " '(',\n",
       " 'NL',\n",
       " '##G',\n",
       " ')',\n",
       " 'with',\n",
       " 'over',\n",
       " '32',\n",
       " '+',\n",
       " 'pre',\n",
       " '##tra',\n",
       " '##ined',\n",
       " 'models',\n",
       " 'in',\n",
       " '100',\n",
       " '+',\n",
       " 'languages',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'inter',\n",
       " '##oper',\n",
       " '##ability',\n",
       " 'between',\n",
       " 'Ten',\n",
       " '##sor',\n",
       " '##F',\n",
       " '##low',\n",
       " '2',\n",
       " '.',\n",
       " '0',\n",
       " 'and',\n",
       " 'P',\n",
       " '##y',\n",
       " '##T',\n",
       " '##or',\n",
       " '##ch',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(\"How many pretrained models are available in Transformers?\", text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# Equivalent to defining first a vector of the inputs, how long and where the sep token is located / called\n",
    "answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "answer\n",
    "tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] How many pretrained models are available in Transformers? [SEP] [UNK] Transformers ( formerly known as pytorch - transformers and pytorch - pretrained - bert ) provides general - purpose architectures ( BERT, GPT - 2, RoBERTa, XLM, DistilBert, XLNet â€¦ ) for Natural Language Understanding ( NLU ) and Natural Language Generation ( NLG ) with over 32 + pretrained models in 100 + languages and deep interoperability between TensorFlow 2. 0 and PyTorch. [SEP]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To fine-tune on SQuAD, run\n",
    "\n",
    "[https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py)\n",
    "\n",
    "  [https://github.com/google-research/bert/blob/master/run_squad.py](https://github.com/google-research/bert/blob/master/run_squad.py)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad', return_dict=True)\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6795, -3.8380, -6.4174, -5.7717, -7.4343, -7.6056, -6.1904, -5.6268,\n",
       "         -5.3795, -6.5965, -4.1058, -4.5886, -1.8644, -1.0924, -3.5250, -3.4141,\n",
       "         -5.5243, -6.4417,  0.0673, -4.3859, -5.0666, -3.7565, -4.6029, -3.2577,\n",
       "         -4.7540, -5.2575,  0.7586, -4.6094, -5.3793, -4.1280, -4.7538, -2.8453,\n",
       "         -5.5077, -5.1051, -5.2773, -4.2016, -5.1009, -3.2573, -2.8114, -1.0467,\n",
       "         -5.5897, -4.6258, -3.3316, -4.9376, -0.7163,  1.3645, -4.3429, -4.1639,\n",
       "         -4.4820, -1.4840, -4.5714, -5.3030, -3.9374, -4.9646, -2.2356, -5.0848,\n",
       "         -5.1845, -5.7979, -5.0689, -4.9632, -3.2077, -5.3158, -5.2520, -3.7048,\n",
       "         -6.3534, -5.7309, -6.1014, -5.6059, -5.4818, -2.5921, -5.5769, -5.3902,\n",
       "         -3.8664, -4.3630, -3.3337,  0.5424, -4.2209, -3.5774, -3.7752, -1.7529,\n",
       "         -5.3554, -3.5998, -4.4433,  0.7994, -3.8337, -3.7183, -3.5382, -1.4980,\n",
       "         -5.4940, -4.1144, -3.2679, -2.1713, -2.1325, -4.0288, -3.1973, -6.4203,\n",
       "         -5.8919, -4.0308, -4.7655, -2.4504, -4.7975, -3.4493, -1.7046,  1.2013,\n",
       "          1.6588, -2.1174, -1.6956,  4.9864, 11.1401,  1.2953,  0.5976,  0.7751,\n",
       "          1.5356, -0.4838,  0.2857, -1.8985,  4.3691, -1.6242, -2.3939, -3.3990,\n",
       "         -1.5266, -2.3745, -4.5886]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_logits'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
